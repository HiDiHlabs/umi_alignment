import os
import pandas as pd
from pathlib import Path
from itertools import product

# print(config)
wrkdir = Path(config['work_dir'])
logdir = Path(config['log_dir'])

metadata = pd.read_csv(config['metadata'])
metadata = metadata[metadata['SAMPLE_NAME']==config['sample']]

# creating file links so as to get a proper directory structure

# fastq_dir = wrkdir / 'fastq'
# os.makedirs(fastq_dir, exist_ok=True)


# for index, row in metadata.iterrows():
#     fastq_file=Path(row['FASTQ_FILE'])
#     suffix='fastq'
#     if '.gz' in row['FASTQ_FILE']:
#         suffix+='.gz'
#     # FILES_BEFORE_MERGEa.append()
#     output_file=fastq_dir / row['RUN_ID'] / (row['SAMPLE_NAME']+'_'+row['READ']+'_'+row['LANE_NO']+'.'+suffix) 
#     os.makedirs(fastq_dir / row['RUN_ID'], exist_ok=True)
#     if os.path.exists(output_file):
#         os.remove(output_file)
#     os.symlink(fastq_file, output_file)
    
# populate the adapter sequences

lib_prep_kit = metadata['LIB_PREP_KIT'].tolist()[0]
adapter_seq_r1 = config[lib_prep_kit+'_R1']
adapter_seq_r3 = config[lib_prep_kit+'_R3']

# populate the lane wildcard

LANE=metadata['LANE_NO'].unique().tolist()
RUN_ID=metadata['RUN_ID'].unique().tolist()

def filter_combinator(combinator, allow_list):
    def filtered_combinator(*args, **kwargs):
        for wc_comb in combinator(*args, **kwargs):
            if frozenset(wc_comb) in allow_list:
                # print(wc_comb)
                yield wc_comb
    return filtered_combinator

allow_list = set()
for run_id in RUN_ID:
    for lane in metadata[metadata.RUN_ID==run_id]['LANE_NO'].unique().tolist():
        allow_list.add(frozenset({("run_id", run_id), ('sample', config['sample']), ("lane", lane)}))


# print(allow_list)



rule all:
    input:
        expand(wrkdir / 'metrics' / '{sample}.mosdepth.global.dist.txt',sample=config['sample']),
        expand(wrkdir / "metrics" / '{sample}.flagstat' , sample=config['sample']),
        expand(wrkdir / "metrics" / "{sample}_insert_size_metrics.txt",sample=config['sample']),
        expand(wrkdir / "metrics" / "{sample}_insert_size.pdf",sample=config['sample']),
        expand(wrkdir / 'metrics'/  '{sample}.mosdepth.summary.txt',sample=config['sample']),
        expand(wrkdir / "metrics" / "{sample}_coverage.png",sample=config['sample']),
        expand(wrkdir / "alignments" / '{sample}_dedup.recall.bam',sample=config['sample']),
        expand(wrkdir / "metrics" / "{sample}.grouped-family-sizes.txt", sample=config['sample']),
        expand(wrkdir / "metrics" / '{sample}_recal_data.table', sample=config['sample']),
        expand(wrkdir / "metrics" / '{sample}_recal_plots.pdf', sample=config['sample']),
        expand(wrkdir / "metrics" / '{sample}_covariates.pdf', sample=config['sample'])

genome = config['genome_human'] # this is a temporary fix, will need to integrate different genomes at a later stage


filtered_product = filter_combinator(product, allow_list) # something to itegrate later which is to allow for run id and lane to have the correct wildcards




rule create_adapter_fastq:
    params:
        adapt_1=adapter_seq_r1,
        adapt_3=adapter_seq_r3
    output:
        adapt_1=temp(wrkdir / '{sample}' / 'cutadapt' / 'adapt_1.fastq'),
        adapt_3=temp(wrkdir / '{sample}' / 'cutadapt' / 'adapt_3.fastq'),
    threads: 1
    resources:
        mem_mb=1000,
        runtime=20,
        nodes=1
    # conda:
    #     "envs/python.yaml"
    run:
        with open(output.adapt_1, 'w') as handle:
            count=1
            for i in params.adapt_1:
                handle.write('>adapter_'+str(count)+'\n')
                handle.write(i+'\n')
                count+=1
                
        with open(output.adapt_3, 'w') as handle:
            count=1
            for i in params.adapt_3:
                handle.write('>adapter_'+str(count)+'\n')
                handle.write(i+'\n')
                count+=1

rule create_links_files:
    params:
        metadata=config['metadata'],
        fastq_dir=wrkdir / 'fastq'
    resources:
        mem_mb=1000,
        runtime=20,
        nodes=1
    output:
        fastq_r1=expand(wrkdir / 'fastq' / '{run_id}' / '{sample}_R1_{lane}.fastq.gz',filtered_product, run_id=RUN_ID, sample=config['sample'], lane=LANE),
        fastq_r2=expand(wrkdir / 'fastq' / '{run_id}' / '{sample}_R2_{lane}.fastq.gz',filtered_product, run_id=RUN_ID, sample=config['sample'], lane=LANE),
        fastq_r3=expand(wrkdir / 'fastq' / '{run_id}' / '{sample}_R3_{lane}.fastq.gz',filtered_product, run_id=RUN_ID, sample=config['sample'], lane=LANE)

    run:
        metadata = pd.read_csv(params.metadata)
        metadata = metadata[metadata['SAMPLE_NAME']==config['sample']]

        for index, row in metadata.iterrows():
            fastq_file=Path(row['FASTQ_FILE'])
            suffix='fastq'
            if '.gz' in row['FASTQ_FILE']:
                suffix+='.gz'
            output_file=params.fastq_dir / row['RUN_ID'] / (row['SAMPLE_NAME']+'_'+row['READ']+'_'+row['LANE_NO']+'.'+suffix) 
            os.makedirs(params.fastq_dir / row['RUN_ID'], exist_ok=True)
            if os.path.exists(output_file):
                os.remove(output_file)
            os.symlink(fastq_file, output_file)
                    
        
        
rule cutadapt:
    input:
        adapt_1=wrkdir / '{sample}' / 'cutadapt' / 'adapt_1.fastq',
        adapt_3=wrkdir / '{sample}' / 'cutadapt' / 'adapt_3.fastq',
        fastq_r1=wrkdir / 'fastq' / '{run_id}' / '{sample}_R1_{lane}.fastq.gz',
        fastq_r3=wrkdir / 'fastq' / '{run_id}' / '{sample}_R3_{lane}.fastq.gz',
    output:
        fastq_r1=temp(wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R1_{lane}_trim.fastq.gz'),
        fastq_r3=temp(wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R3_{lane}_trim.fastq.gz'),
    log:
        logdir / "cutadapt/{run_id}_{sample}_R1_{lane}.log"  
    threads: 8
    resources:
        mem_mb=8000,
        runtime=4*60,
        nodes=1
    conda:
        "envs/cutadapt.yaml"
    shell: "cutadapt -j 8 -a file:{input.adapt_1} -A file:{input.adapt_3} -o {output.fastq_r1} -p {output.fastq_r3} {input.fastq_r1} {input.fastq_r3} &> {log}"

rule fastqbam:
    input:
        genome=genome,
        fastq_r1=wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R1_{lane}_trim.fastq.gz',
        fastq_r3=wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R3_{lane}_trim.fastq.gz',
    output:
        temp(wrkdir / "fastq" / '{run_id}'/ '{sample}_{lane}_unmapped.bam')
    params:
        library="library1"
    threads: 8
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    conda:
        "envs/fgbio.yaml"
    log:
        logdir / "fgbio" / "fastqtobam_{run_id}_{sample}_R1_{lane}.log"
    shell: 
        " ( "
        " fgbio -Xmx1g --compression 1 FastqToBam "
        " --input {input.fastq_r1} {input.fastq_r3} "
        " --sample {wildcards.sample} "
        " --library {params.library} "
        " --output {output} "
        " ) &> {log}"

rule bwa_map:
    input:
        genome=genome,
        bam = wrkdir / "fastq" / '{run_id}'/ '{sample}_{lane}_unmapped.bam'
        # fastq_r1=wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R1_{lane}_trim.fastq.gz',
        # fastq_r3=wrkdir / 'fastq' / '{run_id}' / 'cutadapt' / '{sample}_R3_{lane}_trim.fastq.gz',
    output:
        temp(wrkdir / "alignments" / '{run_id}'/ '{sample}_aln_{lane}.bam')
    threads: 8
    resources:
        mem_mb=10000,
        runtime=24*60,
        nodes=1
    conda:
        "envs/fgbio.yaml"
    log:
        logdir / "bwa/{run_id}_{sample}_R1_{lane}.log"
    shell: 
        " ( "
        " samtools fastq {input.bam} "
        " | bwa mem -t {threads} -p {input.genome} - "
        " | fgbio -Xmx4G --compression 1 --async-io ZipperBams "
        " --unmapped {input.bam} "
        " --ref {input.genome} "
        " --output {output} "
        " )  &> {log}"

rule fgbio:
    input:
        alignment=wrkdir / "alignments" / '{run_id}'/ '{sample}_aln_{lane}.bam',
        fastq_r2=wrkdir / 'fastq' / '{run_id}' / '{sample}_R2_{lane}.fastq.gz',
    output:
        temp(wrkdir / "alignments" / '{run_id}'/ '{sample}_aln_{lane}_umi_annot.bam')
    threads: 1
    resources:
        mem_mb=8000,
        runtime=4*60,
        nodes=1
    conda:
        "envs/fgbio.yaml"
    log:
        logdir / "fgbio/{run_id}_{sample}_R1_{lane}.log"
    shell: "fgbio AnnotateBamWithUmis -i {input.alignment} -f {input.fastq_r2} -o {output} -t RX -q UQ -s true &> {log}"

rule merge:
    input:
        expand(wrkdir / "alignments" / '{run_id}'/ '{sample}_aln_{lane}_umi_annot.bam', filtered_product,  run_id=RUN_ID, sample=config['sample'], lane=LANE),
    output:
        temp(wrkdir / "alignments" / '{sample}_merged_umi_annot.bam'), 
    threads: 8
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    conda:
        "envs/samtools.yaml"
    log:
        logdir / "samtools/{sample}_merge.log"
    shell: "samtools merge -f {output} {input} &> {log}"
    

rule fix_mate:
    """Group the raw reads by UMI and position ready for consensus calling."""
    input:
        bam = wrkdir / "alignments" / '{sample}_merged_umi_annot.bam',
    output:
        bam = temp(wrkdir / "alignments" / '{sample}_mate_fix.bam'),
    params:
        allowed_edits = 1,
    threads:
        2
    resources:
        mem_gb = 8
    log:
        logdir / "fgbio/fixmate_{sample}.log"  
    shell:
        "fgbio -Xmx8g --compression 1 --async-io SetMateInformation "
        "--input {input.bam} "
        "--output {output.bam} "
        
        "&> {log} "


rule group_reads:
    """Group the raw reads by UMI and position ready for consensus calling."""
    input:
        bam = wrkdir / "alignments" / '{sample}_mate_fix.bam',
    output:
        bam = temp(wrkdir / "alignments" / '{sample}_merged_aln_umi_annot_sorted_grouped.bam'),
        stats = wrkdir / "metrics" / "{sample}.grouped-family-sizes.txt"
    params:
        allowed_edits = 1,
    threads:
        2
    resources:
        mem_gb = 8
    log:
        logdir / "fgbio/group_{sample}.log"  
    shell:
        "fgbio -Xmx8g --compression 1 --async-io GroupReadsByUmi "
        "--input {input.bam} "
        "--strategy Adjacency "
        "--edits {params.allowed_edits} "
        "--output {output.bam} "
        "--family-size-histogram {output.stats} "
        "&> {log} "

rule call_consensus_reads:
    """Call consensus reads from the grouped reads."""
    input:
        bam = wrkdir / "alignments" / '{sample}_merged_aln_umi_annot_sorted_grouped.bam',
    output:
        bam = temp(wrkdir / "alignments" / '{sample}.cons.unmapped.bam'),
    params:
        min_reads = 3,
        min_base_qual = 20
    threads:
        4
    resources:
        mem_gb = 8
    log:
        logdir / "fgbio/call_consensus_reads.{sample}.log"
    shell:
        "fgbio -Xmx4g --compression 0 CallMolecularConsensusReads "
        "  --input {input.bam} "
        "  --output {output.bam} "
        "  --min-reads {params.min_reads} "
        "  --min-input-base-quality {params.min_base_qual} "
        "  --threads {threads} "
        "  &> {log}"

rule filter_consensus_reads:
    """Filters the consensus reads and then sorts into coordinate order."""
    input:
        bam = wrkdir / "alignments" / '{sample}.cons.unmapped.bam',
        ref = genome,
    output:
        bam = temp(wrkdir / "alignments" / '{sample}.cons.filtered.bam'),

    params:
        min_reads = 3,
        min_base_qual = 40,
        max_error_rate = 0.2
    threads:
        8
    resources:
        mem_gb = 8
    log:
        logdir / "fgbio" / "filter_consensus_reads.{sample}.log"
    shell:
        " ( "
        " samtools sort -n -u {input.bam} | "
        " fgbio -Xmx8g --compression 1 FilterConsensusReads "
        "   --input /dev/stdin "
        "   --output {output.bam} "
        "   --ref {input.ref} "
        "   --min-reads {params.min_reads} "
        "   --min-base-quality {params.min_base_qual} "
        "   --max-base-error-rate {params.max_error_rate} "
        " ) &> {log} "

rule realign:
    input:
        bam = wrkdir / "alignments" / '{sample}.cons.filtered.bam',
        ref = genome,
    output:
        bam = temp(wrkdir / "alignments" / '{sample}.cons.filtered.realigned.bam'),
        bai = temp(wrkdir / "alignments" / '{sample}.cons.filtered.realigned.bam.bai'),
    threads: 8
    resources:
        mem_mb=10000,
        runtime=24*60,
        nodes=1
    log:
        logdir / "bwa/{sample}_realign.log"
    shell: 
        " ( "
        " samtools fastq {input.bam} "
        " | bwa mem -t {threads} -p {input.ref} - "
        " | fgbio -Xmx4g --compression 0 --async-io ZipperBams "
        " --unmapped {input.bam} "
        " --ref {input.ref} "
        " --tags-to-reverse Consensus "
        " --tags-to-revcomp Consensus "
        " | samtools sort --threads 8 -o {output.bam}##idx##{output.bai} "
        " ) &> {log} "
    




        
rule duplicates:
    input:
        wrkdir / "alignments" / '{sample}.cons.filtered.realigned.bam',
    output:
        bam = wrkdir / "alignments" / '{sample}_dedup.bam',
        bai = wrkdir / "alignments" / '{sample}_dedup.bam.bai',
        metric = wrkdir / "metrics" / '{sample}_marked_dup_metrics.txt'
    conda:
        "envs/picard.yaml"
    threads: 8
    log:
        logdir / "picard/{sample}_dedup.log"
    resources:
        mem_mb=60000,
        runtime=24*60,
        nodes=1
    shell: 
        " ( "
        " picard -Xmx60G MarkDuplicates -I {input} "
        " -O {output.bam} -M {output.metric} --BARCODE_TAG RX "
        " --SORTING_COLLECTION_SIZE_RATIO 0.1 && "
        " samtools index -b {output.bam} {output.bai} "
        " ) &> {log} "

rule baseRecalibrator:
    input:
        bam = wrkdir / "alignments" / '{sample}_dedup.bam',
        dbsnp = config['dbsnp'],
        genome = genome,
    output:
        table = wrkdir / "metrics" / '{sample}_recal_data.table',
        plots = wrkdir / "metrics" / '{sample}_recal_plots.pdf',
        bam = wrkdir / "alignments" / '{sample}_dedup.recall.bam',
        analyse_covariates = wrkdir / "metrics" / '{sample}_covariates.pdf'
    conda:
        "envs/gatk.yaml"
    threads: 8
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    log:
        logdir / "gatk/{sample}_recal.log"
    shell: 
        " ( "
        " gatk BaseRecalibrator -I {input.bam} -R {input.genome} "
        " --known-sites {input.dbsnp} "
        " -O {output.table} && "
        " gatk ApplyBQSR -I {input.bam} -R {genome} --bqsr-recal-file {output.table} -O {output.bam} && "
        " gatk AnalyzeCovariates "
        " -bqsr {output.table} "
        " -plots {output.analyse_covariates} "
        " ) &> {log} "

rule flagstatt:
    input:
        bam= wrkdir / "alignments" / '{sample}_dedup.recall.bam',
    output:
        wrkdir / "metrics" / "{sample}.flagstat"
    conda:
        "envs/sambamba.yaml"
    threads: 1
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    log:
        logdir / "sambamba/{sample}.log"
    shell: "sambamba flagstat {input.bam} > {output} &> {log}"
    
        
rule mosdepth:
    input:
        bam = wrkdir / "alignments" / '{sample}_dedup.recall.bam',
    params:
        prefix=str(wrkdir / 'metrics'/ '{sample}' )
    output:
        out_1=wrkdir / 'metrics' / '{sample}.mosdepth.global.dist.txt',
        out_2=wrkdir / 'metrics'/  '{sample}.mosdepth.summary.txt'
    threads: 1
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    conda:
        "envs/mosdepth.yaml"
    log:
        logdir / "mosdepth/{sample}.log"
        
    shell: "mosdepth -n {params.prefix} {input.bam} &> {log}"

rule InsertSize:
    input:
        bam= wrkdir / "alignments" / '{sample}_dedup.recall.bam',
    output:
        size_metric=wrkdir / "metrics" / "{sample}_insert_size_metrics.txt",
        pdf=wrkdir / "metrics" / "{sample}_insert_size.pdf"
    conda:
        "envs/picard.yaml"
    threads: 1
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    log:
        logdir / "picard/{sample}_insert_size.log"
    shell: "picard -Xmx8G CollectInsertSizeMetrics -I {input.bam} -O {output.size_metric} -H {output.pdf} &> {log}"

rule coveragePlot:
    input:
        bam = wrkdir / "alignments" / '{sample}_dedup.recall.bam',
    params:
        binsize = 50
    output:
        plot= wrkdir / "metrics" / "{sample}_coverage.png"
    threads: 20
    resources:
        mem_mb=8000,
        runtime=24*60,
        nodes=1
    conda:
        "envs/coveragePlot.yaml"
    log:
        logdir / "coveragePlot/{sample}.log"
    script:
        "scripts/getCoveragePlot_snakemake.R"